{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6257b28e-ed21-4f08-b7cc-0992ca16654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a93428-a5c6-4867-87d6-6350443185bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to crop all images to this resolution\n",
    "image_resolution = 64\n",
    "\n",
    "# how many colour channels?\n",
    "# 3 for RGB, 1 for greyscale\n",
    "colour_channels = 3\n",
    "\n",
    "# make sure the path to your image folder is correct\n",
    "folder_path = './data/my_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dffa0769-dc82-4425-8871-755f4d4d623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.v2 import Compose, ToImage, ToDtype, Grayscale, Resize, RandomCrop, Normalize\n",
    "\n",
    "# this transformation function will help us pre-process images during the training (on-the-fly)\n",
    "transformation = Compose([   \n",
    "    # convert an image to tensor\n",
    "    ToImage(),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "    \n",
    "    # resize and crop\n",
    "    Resize(image_resolution),\n",
    "    RandomCrop(image_resolution),\n",
    "    \n",
    "    # normalise pixel values to be between -1 and 1 \n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    \n",
    "    # if you're training on greyscale images, convert them to greyscale\n",
    "    # otherwise just do nothing\n",
    "    Grayscale() if colour_channels == 1 else torch.nn.Identity()\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d07881b-cd2e-4ee9-be01-dc56dfc7471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda59b77-0d33-4151-85e3-84ccf79f8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many images are going to be put into the testing set, \n",
    "# e.g. 0.2 means 20% percent of images\n",
    "test_size = 0.2 \n",
    "\n",
    "# how many images will be used in one epoch, \n",
    "# this usually depend on your model / types of data / CPU or GPU's capability\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9064271-cf70-44c7-9487-0cbeac5d4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate train and test datasets\n",
    "train_dataset = ImageFolder(folder_path, transform=transformation)\n",
    "test_dataset = ImageFolder(folder_path, transform=transformation)\n",
    "\n",
    "# Get length of dataset and indicies\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "\n",
    "# Get train / test split for data points\n",
    "train_indices, test_indices = train_test_split(indices, test_size=test_size, random_state=42)\n",
    "\n",
    "# Override dataset classes to only be samples for each split\n",
    "train_sub_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "test_sub_dataset = torch.utils.data.Subset(test_dataset, test_indices)\n",
    "\n",
    "# Create training and tresing data loaders\n",
    "train_loader = DataLoader(train_sub_dataset, batch_size=batch_size, num_workers=2, multiprocessing_context=\"forkserver\" if device=='mps' else None, shuffle=True)\n",
    "test_loader = DataLoader(test_sub_dataset, batch_size=batch_size, num_workers=2, multiprocessing_context=\"forkserver\" if device=='mps' else None, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1b42f-3690-4ce3-a4da-49f225ab6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort out the names for each classes (according to the folder names)\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print(f'{len(train_indices)} training images loaded')\n",
    "print(f'{len(test_indices)} testing images loaded')\n",
    "print(f'classes: {class_names}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106f7fb-ab35-484d-9aa0-383371701a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = next(iter(train_loader))\n",
    "\n",
    "print(f'data shape: {data.shape}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80e7a9-13a7-46b2-a74f-adea1813acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a grid of images and display them\n",
    "\n",
    "grid = make_grid(data, nrow = 8)\n",
    "display(to_pil_image(grid.add(1).div(2)))\n",
    "\n",
    "print([f'{i}: {class_names[class_name]}' for i, class_name in enumerate(labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "820d866f-ed3c-48a9-841c-61635f37b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import ConvNeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796cb1e-d300-43a5-88ae-cd2167b8008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNeuralNetwork(img_channel = colour_channels, \n",
    "                          img_resolution = image_resolution,\n",
    "                          num_classes = len(class_names))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd23048f-221b-462f-955b-3d51485a9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad26605-d776-427d-bb75-53f0a67ce584",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can save the model regularly\n",
    "save_every_n_epoch = 5\n",
    "\n",
    "# total number of epochs we aim for\n",
    "num_epochs = 8\n",
    "\n",
    "# keep track of the losses, we can plot them in the end\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print('Epoch 0')\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "\n",
    "    #---- Training loop -----------------------------\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Load: The training data loader loads a batch of training data and their true class labels.\n",
    "        inputs, true_labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "        \n",
    "        # Pass: Forward pass the training data to our model, and get the predicted classes.\n",
    "        pred_labels = model(inputs)\n",
    "        \n",
    "        # Loss: The loss function compares the predicted classes to the true classes, and calculates the error.\n",
    "        loss = loss_function(pred_labels, true_labels)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Optimise: The optimizer slightly optimises our model based on the error.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f'  -> Step {i + 1:04}, train loss: {loss.item():.4f}')\n",
    "    \n",
    "    \n",
    "    #---- Testing loop -----------------------------\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        test_loss = 0.0\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            # Load: The testing data loader loads a batch of testing data and their true class labels.\n",
    "            inputs, true_labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            true_labels = true_labels.to(device)\n",
    "            \n",
    "            # Pass: Forward pass the testing data to our model, and get the predicted classes.\n",
    "            pred_labels = model(inputs)\n",
    "            \n",
    "            # Loss: The loss function compares the predicted classes to the true classes, and calculates the error.\n",
    "            loss = loss_function(pred_labels, true_labels)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    #---- Report some numbers -----------------------------\n",
    "    \n",
    "    # Calculate the cumulative losses in this epoch\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Added cumulative losses to lists for later display\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, train loss: {train_loss:.3f}, test loss: {test_loss:.3f}')\n",
    "    \n",
    "    # save our model every n epoch\n",
    "    if (epoch+1) % save_every_n_epoch==0:\n",
    "        torch.save(model.state_dict(), f'model_epoch{epoch:04}.pt')\n",
    "        \n",
    "# save the model at the end of the training process\n",
    "torch.save(model.state_dict(), f'model_final.pt')\n",
    "\n",
    "print(\"training finished, model saved to 'model_final.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "917370e6-d946-4204-9a0e-cfc65f8c7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f172f2-56c9-4539-95e7-115a46679c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(train_losses, label='training loss')\n",
    "plt.plot(test_losses, label = 'validation loss')\n",
    "plt.xticks(np.arange(len(train_losses)))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17404949-683e-4893-aa6c-71ddeecf1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import ConvNeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3370865e-e43f-499a-8d21-2ee9116ff0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the parameters are the same as when the model is created\n",
    "eval_model = ConvNeuralNetwork(img_channel = colour_channels, \n",
    "                          img_resolution = image_resolution,\n",
    "                          num_classes = len(class_names))\n",
    "\n",
    "# load the saved model, make sure the path is correct\n",
    "eval_model.load_state_dict(torch.load('model_final.pt'))\n",
    "\n",
    "eval_model.to(device)\n",
    "eval_model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9577247c-0e90-41e9-8804-5e8b59b1d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = next(iter(test_loader))\n",
    "\n",
    "true_label = class_names[test_labels[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300eacb-787e-4a40-a131-9086d6465c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.title(f\"True Label: {true_label}\")\n",
    "plt.imshow(test_data[0].add(1).div(2).permute((1,2,0)).cpu().detach().numpy())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "608d7d15-58ad-41d1-a28b-2dc0a3c0afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    pred_labels = eval_model(test_data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d65cdfc-f7d3-4591-a924-0e9f22e6ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,2))\n",
    "plt.plot(pred_labels[0].cpu().detach().numpy())\n",
    "plt.xticks(np.arange(len(class_names)),class_names, rotation='vertical')\n",
    "plt.ylabel(\"Probabilities\")\n",
    "plt.grid(axis='x', color='0.95')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad3787-e8b4-4a3f-8326-3bac8c8919dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted_class_index = torch.max(pred_labels[0], 0)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.title(f\"True label: {true_label}, predicted label: {class_names[predicted_class_index]}\")\n",
    "plt.imshow(test_data[0].add(1).div(2).permute((1,2,0)).cpu().detach().numpy())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4845d33-2268-491d-b2c9-4cba169b1afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 0\n",
    "num_correct = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        # Load: The testing data loader loads a batch of testing data and their true class labels.\n",
    "        inputs, true_labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "\n",
    "        # Pass: Forward pass the testing data to our model, and get the predicted classes.\n",
    "        pred_labels = eval_model(inputs)\n",
    "        pred_labels = torch.argmax(pred_labels, dim=1)\n",
    "        \n",
    "        num_correct += pred_labels.size(0) - torch.count_nonzero(pred_labels - true_labels)\n",
    "        num_samples += pred_labels.size(0) \n",
    "        \n",
    "accuracy = num_correct / num_samples\n",
    "print(f'correct samples: {num_correct}  \\ntotal samples: {num_samples}  \\nmodel accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db9363d-c866-42fa-b4c9-867b8585c43c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
