{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192ed1fb-aeb4-4cf7-8798-3099e147dd25",
   "metadata": {},
   "source": [
    "# Train an image classifier\n",
    "\n",
    "This notebook demonstrates training a [Convolutional Neural Network (CNN)](https://www.ibm.com/topics/convolutional-neural-networks) to classify images. There'll be two options: \n",
    "\n",
    " **Option 1** - Train a image classifier using Pinterest images we downloaded from the `01_download_images.ipynb` notebook.  \n",
    " **Option 2** - Train a sketch classifier using a small subset of Google's [Quick, Draw!](https://quickdraw.withgoogle.com/data) dataset.\n",
    "\n",
    "It will take the following setps:\n",
    "\n",
    "**Step 0** - Prepare images  \n",
    "**Step 1** - Load your dataset  \n",
    "**Step 2** - Create an empty neural network model  \n",
    "**Step 3** - Train the model  \n",
    "**Step 4** - Test and evaluate the training results  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a083617-3886-4b5b-b7cb-bf0396afa64a",
   "metadata": {},
   "source": [
    "Before we start, let's config the device PyTorch is using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6257b28e-ed21-4f08-b7cc-0992ca16654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f7585-5854-494a-b9aa-cc1fc7116d91",
   "metadata": {},
   "source": [
    "## **Step 0** - Prepare images\n",
    "\n",
    "\n",
    "Depending on your choice, follow either **option 1** or **option 2**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ee753-3e26-42a0-9678-71f897fac942",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Option 1** - Using Pinterest images we have downloaded  \n",
    "\n",
    "Instructions for downloading Pinterest images can be found in the `01_download_images.ipynb` notebook.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680a59c-e0b5-41e0-a981-5a81353b2a84",
   "metadata": {},
   "source": [
    "First, make sure you have sorted them into a main folder and respective category folders,   \n",
    "e.g. in your `data` folder under class-3, you should have:  \n",
    "```\n",
    "./my_dataset/chairs/0001.jpg  <- the file name 001.jpg can be anything\n",
    "./my_dataset/chairs/0002.jpg\n",
    "...\n",
    "./my_dataset/dogs/0001.jpg\n",
    "./my_dataset/dogs/0002.jpg\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "Then, define some configurations for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a93428-a5c6-4867-87d6-6350443185bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to crop all images to this resolution\n",
    "image_resolution = 64\n",
    "\n",
    "# how many colour channels?\n",
    "# 3 for RGB, 1 for greyscale\n",
    "colour_channels = 3\n",
    "\n",
    "# make sure the path to your image folder is correct\n",
    "folder_path = './data/my_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e193e3f-9fe3-475b-b7c7-bc1f36fa049f",
   "metadata": {},
   "source": [
    "Now we have finished Option 1, please skip to **Step 1** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e59033-3174-44d2-8180-d68ce2ee5b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e85182ed-12d7-4567-8166-381a68da0e1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Option 2** - Using sketches from the QuickDraw! dataset\n",
    "\n",
    "\n",
    "I prepared a small subset of Google's [Quick, Draw!](https://quickdraw.withgoogle.com/data) dataset in `./data/quick_draw_small.zip`. This subset contains 5000 greyscale drawings in `.jpg` format in 10 categories. \n",
    "\n",
    "If you choose this option, we'll build and train a model that can classify the category of these drawings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d5380-23d3-4bc8-9d0d-7b1335783271",
   "metadata": {},
   "source": [
    "#### Unzip the dataset folder and verify the data   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317e499-4f04-4eda-ace8-125c9b4c0bd6",
   "metadata": {},
   "source": [
    "##### Mac / Linux users:\n",
    "\n",
    "Use this command to unzip the dataset in the `data` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24187dd9-cb70-4282-ae0d-57d9290ecff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q ./data/quick_draw_small.zip -d ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c7256-7710-4e58-bc83-1e0591993bc2",
   "metadata": {},
   "source": [
    "##### Windows users:  \n",
    "\n",
    "Please manually unzip the `quick_draw_small.zip` file in the `data` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1444ea3-1bc4-40ee-b136-d4ae6f341a48",
   "metadata": {},
   "source": [
    "Now we should see the `quick_draw_small` folder, inside it should have ten folders with class names as folder names. Each folder should have 2000 drawings in `.jpg` format. \n",
    "\n",
    "Then, define some configurations for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49310d82-79e5-4ad2-9322-64ef1233dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to crop all images to this resolution\n",
    "image_resolution = 28\n",
    "\n",
    "# how many colour channels?\n",
    "# 3 for RGB, 1 for greyscale\n",
    "colour_channels = 1\n",
    "\n",
    "# make sure the path to your image folder is correct\n",
    "folder_path = './data/quick_draw_small'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b56f8-af4e-4663-b906-4800c91ca038",
   "metadata": {},
   "source": [
    "\n",
    "## **Step 1** - Load your dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705bee3-b4b8-4cd6-a8b4-bb70719e936e",
   "metadata": {},
   "source": [
    "#### **1.1** - Pre-process images \n",
    "\n",
    "We need to pre-process (i.e. crop, resize, normalise) the images in our dataset.\n",
    "\n",
    "We're going to use [PyTorch's Transforming functions](https://pytorch.org/vision/0.17/transforms.html) to define the pre-processes pipeline on our images. \n",
    "\n",
    "Refer to the documentation if you're unsure about some of the functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dffa0769-dc82-4425-8871-755f4d4d623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.v2 import Compose, ToImage, ToDtype, Grayscale, Resize, RandomCrop, Normalize\n",
    "\n",
    "# this transformation function will help us pre-process images during the training (on-the-fly)\n",
    "transformation = Compose([   \n",
    "    # convert an image to tensor\n",
    "    ToImage(),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "    \n",
    "    # resize and crop\n",
    "    Resize(image_resolution),\n",
    "    RandomCrop(image_resolution),\n",
    "    \n",
    "    # normalise pixel values to be between -1 and 1 \n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    \n",
    "    # if you're training on greyscale images, convert them to greyscale\n",
    "    # otherwise just do nothing\n",
    "    Grayscale() if colour_channels == 1 else torch.nn.Identity()\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597b7d3-05b2-405f-95af-5dfcb3db655d",
   "metadata": {},
   "source": [
    "#### **1.2** - Create dataset loaders  \n",
    "\n",
    "In order to load images during training, we're going to use something called \"data loader\".  \n",
    "These data loaders loads images into batches according to the pre-processes pipeline we have defined previously.\n",
    "\n",
    "We'll be using PyTorch's [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) to create a training data loader and a testing data loader.  \n",
    "\n",
    "> Note: If the following codes give you an error like `No module named 'sklearn'` or `No module named 'matplotlib'`, it means you have not yet installed matplotlib or sklearn. To install it, open up a terminal/command window, activate your environment (e.g., `conda activate coding3`), and then type `conda install matplotlib` and `conda install scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d07881b-cd2e-4ee9-be01-dc56dfc7471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda59b77-0d33-4151-85e3-84ccf79f8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many images are going to be put into the testing set, \n",
    "# e.g. 0.2 means 20% percent of images\n",
    "test_size = 0.2 \n",
    "\n",
    "# how many images will be used in one epoch, \n",
    "# this usually depend on your model / types of data / CPU or GPU's capability\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15382508-f1e2-4336-8927-af012ff1a2b7",
   "metadata": {},
   "source": [
    "There is a lot of discussion about how different batch sizes can affect training results, e.g. [here](https://www.cs.princeton.edu/~smalladi/blog/2024/01/22/SDEs-ScalingRules/). But as a rule of thumb: larger batch sizes can make your gradient less noisy and therefore safer for larger learning rates, but this is always subjected to different models and data, especially when training generative models. So a good way to decide batch sizes is to follow literatures or similar works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9064271-cf70-44c7-9487-0cbeac5d4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate train and test datasets\n",
    "train_dataset = ImageFolder(folder_path, transform=transformation)\n",
    "test_dataset = ImageFolder(folder_path, transform=transformation)\n",
    "\n",
    "# Get length of dataset and indicies\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "\n",
    "# Get train / test split for data points\n",
    "train_indices, test_indices = train_test_split(indices, test_size=test_size, random_state=42)\n",
    "\n",
    "# Override dataset classes to only be samples for each split\n",
    "train_sub_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "test_sub_dataset = torch.utils.data.Subset(test_dataset, test_indices)\n",
    "\n",
    "# Create training and tresing data loaders\n",
    "train_loader = DataLoader(train_sub_dataset, batch_size=batch_size, num_workers=2, multiprocessing_context=\"forkserver\" if device=='mps' else None, shuffle=True)\n",
    "test_loader = DataLoader(test_sub_dataset, batch_size=batch_size, num_workers=2, multiprocessing_context=\"forkserver\" if device=='mps' else None, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1b42f-3690-4ce3-a4da-49f225ab6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort out the names for each classes (according to the folder names)\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print(f'{len(train_indices)} training images loaded')\n",
    "print(f'{len(test_indices)} testing images loaded')\n",
    "print(f'classes: {class_names}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839732ce-c8eb-40dc-a721-4dcdda89a427",
   "metadata": {},
   "source": [
    "#### **1.3** - Visualise our data\n",
    "Now we have the `train_loader` and the `test_loader` objects,  \n",
    "We can use them to load and visualise some of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60ab2b-7293-4c02-b982-52fce6a04dd4",
   "metadata": {},
   "source": [
    "\n",
    "**Questions 1.3**: Use the following three lines of code to load a batch of training data and labels and print their shapes. What are the shapes of your `data` and `labels` variables? `data` and `labels` should be a 4-dimensional tensor and a 1-dimensional tensor. What does each dimension represent?\n",
    "\n",
    "Hint: If you're unsure, have a look at the image tensor example from [last week's notebook](../class-1/pytorch_tensors_fundamentals.ipynb#Introduction-to-tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106f7fb-ab35-484d-9aa0-383371701a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = next(iter(train_loader))\n",
    "\n",
    "print(f'data shape: {data.shape}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80e7a9-13a7-46b2-a74f-adea1813acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a grid of images and display them\n",
    "\n",
    "grid = make_grid(data, nrow = 8)\n",
    "display(to_pil_image(grid.add(1).div(2)))\n",
    "\n",
    "print([f'{i}: {class_names[class_name]}' for i, class_name in enumerate(labels)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640072b-bf47-45a2-83c4-386f6b02f9dc",
   "metadata": {},
   "source": [
    "This is one batch of training data, it contains both the images and their categories, check if they match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e60261d-24d6-44a8-ab7e-981194ff1324",
   "metadata": {},
   "source": [
    "## **Step 2** - Load an empty neural network model   \n",
    "\n",
    "We're going to skip over the details of how to setup a neural network model for now, they will be covered next week. If you want to have a sneak peek, feel free to check the `./src/model.py` file.  \n",
    "\n",
    "For now, trust that the code makes a new neural network according to your dataset configurations.  \n",
    "However, there're three important parameters:\n",
    "\n",
    " - `img_channel` defines the number of colour channels of inputs (1 for greyscale, 3 for RGB).\n",
    " - `img_resolution` defines the width and height of input images. \n",
    " - `num_classes` defines how many categories we are classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "820d866f-ed3c-48a9-841c-61635f37b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import ConvNeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796cb1e-d300-43a5-88ae-cd2167b8008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNeuralNetwork(img_channel = colour_channels, \n",
    "                          img_resolution = image_resolution,\n",
    "                          num_classes = len(class_names))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321d247-0090-40c6-a510-bf20322d40a6",
   "metadata": {},
   "source": [
    "## **Step 3** - Train the model\n",
    "\n",
    "#### **3.1** - Define optimiser and loss function\n",
    "\n",
    "First, let's define an optimizer and the type of error (loss) function:\n",
    " - we use the `adam` optimizer\n",
    " - we use the `cross entropy loss` as our loss function \n",
    " \n",
    "This means: during the training process, the adam optimizer will try to minimise errors calculated by the cross entropy loss ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd23048f-221b-462f-955b-3d51485a9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb2e20-d204-4f62-9c25-0a62ffcc4554",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **3.2** - Break and Recap\n",
    "\n",
    "Up to this point, we have defined a lot of components for the model training.   \n",
    "Let's recap what we have so far:\n",
    " - A training data loader `train_loader` and a testing data loader `test_loader` we defined in **Step 1**.\n",
    " - An empty neural network `model` we built in **Step 2**.\n",
    " - An optimizer `optimizer` and a loss function `loss_function` we just defined.\n",
    "\n",
    "In the next step, we're going to program how they will coordinate during training. So..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c6094-9e6a-4d2c-927e-6dcd09f68d36",
   "metadata": {},
   "source": [
    "#### **3.3** - Define training/ testing loop  \n",
    "\n",
    "As an overview, each epoch has a training loop and a testing loop:   \n",
    "In each training loop:\n",
    " - **Load**: The training data loader loads a batch of training data and their true class labels.\n",
    " - **Forward**: Forward pass the training data to our model, and get the predicted class labels.\n",
    " - **Loss**: Use the loss function to compares the predicted labels to the true labels, and compute the error.\n",
    " - **Optimise**: The optimizer slightly optimises our model based on the error computed by the loss function.\n",
    "\n",
    "Once we finish a training loop, we do a testing loop. In a testing loop, we do exactly the same things as in a training loop, but we won't update our model. The testing results are only used to tell us how well the model is doing at the moment.\n",
    "\n",
    "In each testing loop:\n",
    " - **Load**: The testing data loader loads a batch of testing data and their true class labels.\n",
    " - **Forward**: Forward pass the testing data to our model, and get the predicted class labels.\n",
    " - **Loss**: Use the loss function to compares the predicted labels to the true labels, and calculates the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad26605-d776-427d-bb75-53f0a67ce584",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can save the model regularly\n",
    "save_every_n_epoch = 5\n",
    "\n",
    "# total number of epochs we aim for\n",
    "num_epochs = 8\n",
    "\n",
    "# keep track of the losses, we can plot them in the end\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print('Epoch 0')\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "\n",
    "    #---- Training loop -----------------------------\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Load: The training data loader loads a batch of training data and their true class labels.\n",
    "        inputs, true_labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "        \n",
    "        # Pass: Forward pass the training data to our model, and get the predicted classes.\n",
    "        pred_labels = model(inputs)\n",
    "        \n",
    "        # Loss: The loss function compares the predicted classes to the true classes, and calculates the error.\n",
    "        loss = loss_function(pred_labels, true_labels)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Optimise: The optimizer slightly optimises our model based on the error.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f'  -> Step {i + 1:04}, train loss: {loss.item():.4f}')\n",
    "    \n",
    "    \n",
    "    #---- Testing loop -----------------------------\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        test_loss = 0.0\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            # Load: The testing data loader loads a batch of testing data and their true class labels.\n",
    "            inputs, true_labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            true_labels = true_labels.to(device)\n",
    "            \n",
    "            # Pass: Forward pass the testing data to our model, and get the predicted classes.\n",
    "            pred_labels = model(inputs)\n",
    "            \n",
    "            # Loss: The loss function compares the predicted classes to the true classes, and calculates the error.\n",
    "            loss = loss_function(pred_labels, true_labels)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    #---- Report some numbers -----------------------------\n",
    "    \n",
    "    # Calculate the cumulative losses in this epoch\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Added cumulative losses to lists for later display\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, train loss: {train_loss:.3f}, test loss: {test_loss:.3f}')\n",
    "    \n",
    "    # save our model every n epoch\n",
    "    if (epoch+1) % save_every_n_epoch==0:\n",
    "        torch.save(model.state_dict(), f'model_epoch{epoch:04}.pt')\n",
    "        \n",
    "# save the model at the end of the training process\n",
    "torch.save(model.state_dict(), f'model_final.pt')\n",
    "\n",
    "print(\"training finished, model saved to 'model_final.pt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7019eccc-0d26-4f1c-a88a-87d325cf7e10",
   "metadata": {},
   "source": [
    "#### **3.4** - Plot Training Process\n",
    "\n",
    "Plot how errors (\"loss\") change over time on the training set and the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "917370e6-d946-4204-9a0e-cfc65f8c7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f172f2-56c9-4539-95e7-115a46679c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(train_losses, label='training loss')\n",
    "plt.plot(test_losses, label = 'validation loss')\n",
    "plt.xticks(np.arange(len(train_losses)))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7abc11-74be-4a6a-ad5c-e47cfcdbe58e",
   "metadata": {},
   "source": [
    "**Question 3.4**: Please save and upload the figure into the quiz on Moodle. Could you describe the plotted lines in the figure, and explain what these lines indicate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aec91f-7cb2-47b2-8037-dc96f45f5e79",
   "metadata": {},
   "source": [
    "## **Step 4** - Test and evaluate the training results   \n",
    "\n",
    "#### **4.1** - Load model  \n",
    "First, let's load our trained model. And set the model to 'evaluation' mode `.eval()` (check [torch.nn.Module.eval()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17404949-683e-4893-aa6c-71ddeecf1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import ConvNeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3370865e-e43f-499a-8d21-2ee9116ff0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the parameters are the same as when the model is created\n",
    "eval_model = ConvNeuralNetwork(img_channel = colour_channels, \n",
    "                          img_resolution = image_resolution,\n",
    "                          num_classes = len(class_names))\n",
    "\n",
    "# load the saved model, make sure the path is correct\n",
    "eval_model.load_state_dict(torch.load('model_final.pt'))\n",
    "\n",
    "eval_model.to(device)\n",
    "eval_model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c37508-4e76-4e52-a8d7-70f00caab58a",
   "metadata": {},
   "source": [
    "#### **4.2** - Load some test data \n",
    "\n",
    "Data in the testing set has never been exposed to the model during the training process, so they are good to test if our model works fine on \"new\" data. Why? Because sometimes the model begins to **overfit** to the training set and is not able to \"generalise\" to data that are not in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9577247c-0e90-41e9-8804-5e8b59b1d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = next(iter(test_loader))\n",
    "\n",
    "true_label = class_names[test_labels[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300eacb-787e-4a40-a131-9086d6465c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.title(f\"True Label: {true_label}\")\n",
    "plt.imshow(test_data[0].add(1).div(2).permute((1,2,0)).cpu().detach().numpy())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c3c1f-f696-4429-8836-7fd03f3cd192",
   "metadata": {},
   "source": [
    "Run it through a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "608d7d15-58ad-41d1-a28b-2dc0a3c0afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    pred_labels = eval_model(test_data.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909745f2-0191-48c5-beba-6da75ab0e920",
   "metadata": {},
   "source": [
    "#### **4.3** - Visualise the output\n",
    "\n",
    "`pred_labels` is our model's output. It's not a single class label, instead, it's a set of probabilities indicating how likely the drawing belongs to a class.  \n",
    "\n",
    "We can use the following codes to plot these probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d65cdfc-f7d3-4591-a924-0e9f22e6ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,2))\n",
    "plt.plot(pred_labels[0].cpu().detach().numpy())\n",
    "plt.xticks(np.arange(len(class_names)),class_names, rotation='vertical')\n",
    "plt.ylabel(\"Probabilities\")\n",
    "plt.grid(axis='x', color='0.95')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28039ff-790a-4ac5-8d9b-f2df41b5fc19",
   "metadata": {},
   "source": [
    "I trained a sketch recognition model and I'm using this sketch as the input:\n",
    "\n",
    "\n",
    "<img src='./src/graphics/test_plot.png' width='100px'></img>\n",
    "\n",
    "And the codes above output this:\n",
    "\n",
    "<img src='./src/graphics/prob_plot.png' width='500px'></img>\n",
    "\n",
    "In my example, the category with the highest probability is \"fish\", which means that the model \"thinks\" this sketch is probably a fish.   \n",
    "The second highest probability is \"whale\", indicating the model \"thinks\" it may also be a whale (this make sense because sketches of fish and whale may looks similar).\n",
    "\n",
    "**Question 4.3:** What is the input image you're using? And what are the predicted probabilities? Please save and upload them to the quiz in Moodle. It's okay if it outputs an incorrect prediction, if this happens, could you guess why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad3787-e8b4-4a3f-8326-3bac8c8919dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted_class_index = torch.max(pred_labels[0], 0)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.title(f\"True label: {true_label}, predicted label: {class_names[predicted_class_index]}\")\n",
    "plt.imshow(test_data[0].add(1).div(2).permute((1,2,0)).cpu().detach().numpy())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e285a1-82f5-4ecb-8268-d7cea19e2f61",
   "metadata": {},
   "source": [
    "#### **4.4** - Calculate accuracy  \n",
    "\n",
    "Now we can run our model on the full testing set to calculate the accuracy of our model.  \n",
    "In practice, we'll keep counting the correct predictions and check what percentage of the predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4845d33-2268-491d-b2c9-4cba169b1afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 0\n",
    "num_correct = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        # Load: The testing data loader loads a batch of testing data and their true class labels.\n",
    "        inputs, true_labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "\n",
    "        # Pass: Forward pass the testing data to our model, and get the predicted classes.\n",
    "        pred_labels = eval_model(inputs)\n",
    "        pred_labels = torch.argmax(pred_labels, dim=1)\n",
    "        \n",
    "        num_correct += pred_labels.size(0) - torch.count_nonzero(pred_labels - true_labels)\n",
    "        num_samples += pred_labels.size(0) \n",
    "        \n",
    "accuracy = num_correct / num_samples\n",
    "print(f'correct samples: {num_correct}  \\ntotal samples: {num_samples}  \\nmodel accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db9363d-c866-42fa-b4c9-867b8585c43c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
